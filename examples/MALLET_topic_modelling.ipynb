{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling Tal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to test how [Taylor Salo](https://www.github.com/tsalo) integrated Mallet into NeuroSynth, and whether that integration works in a docker container.\n",
    "\n",
    "First, let's import some dependencies and text to work with. \n",
    "\n",
    "For testing, we'll use an XML file separately downloaded from PubMed. In the spirit of NeuroSynth, we downloaded [Tal Yarkoni's](https://www.ncbi.nlm.nih.gov/pubmed/?term=tal+yarkoni) bibliography. Thanks, Tal! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-24dc38a8cada>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neurosynth/tests/data/pubmed_result.xml'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mxml_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fmri/anaconda3/envs/py27/lib/python2.7/site-packages/bs4/__init__.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     % \",\".join(features))\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             if not (original_features == builder.NAME or\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "with open('neurosynth/tests/data/pubmed_result.xml') as infile:\n",
    "    xml_file = infile.read()\n",
    "soup = BeautifulSoup(xml_file, 'lxml')\n",
    "\n",
    "try:\n",
    "    assert type(soup) == BeautifulSoup\n",
    "except AssertionError:\n",
    "    print('Check file type! Must be HTML or XML.')\n",
    "\n",
    "titles = soup.find_all('articletitle')\n",
    "abstracts = soup.find_all('abstract')\n",
    "\n",
    "if len(titles) != len(abstracts):\n",
    "    print('Warning: Some articles do not have abstracts on PubMed!')\n",
    "    print('Only articles with complete data will be included.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three articles do not have abstracts:\n",
    "1. Pain in the ACC?\n",
    "2. Introduction to the special issue on reliability and \n",
    "    replication in cognitive and affective neuroscience research.\n",
    "3. Establishing homology between monkey and human brains.\n",
    "\n",
    "Maybe because they're commentaries? Regardless, we'll need to filter the results to only consider articles with abstracts. Then, import any matching articles into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "pmids = []\n",
    "\n",
    "articles = soup.find_all('pubmedarticle')\n",
    "for a in articles:\n",
    "    if a.find_all('abstract')!= []:\n",
    "        # This is a little messy, but pulls out the\n",
    "        # results in plain text without another loop.\n",
    "        abstracts.append(a.find_all('abstracttext')[0].get_text())\n",
    "        pmids.append(a.find_all(idtype='pubmed')[0].get_text())\n",
    "\n",
    "df = pd.DataFrame({'pmid': pmids,\n",
    "     'abstract': abstracts})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a test dataset! Let's see how it plays with Mallet, outside of Docker first. \n",
    "\n",
    "To do so, I'll just copy Taylor's function directly along with some dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taylor's function:\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "def topic_models(abstracts, n_topics=50, n_words=31, n_iters=1000, alpha=None,\n",
    "                  beta=0.001):\n",
    "     \"\"\" Perform topic modeling using Latent Dirichlet Allocation with the\n",
    "     Java toolbox MALLET.\n",
    "     \n",
    "     Args:\n",
    "         abstracts:  A pandas DataFrame with two columns ('pmid' and 'abstract')\n",
    "                     containing article abstracts.\n",
    "         n_topics:   Number of topics to generate. Default=50.\n",
    "         n_words:    Number of top words to return for each topic. Default=31,\n",
    "                     based on Poldrack et al. (2012).\n",
    "         n_iters:    Number of iterations to run in training topic model.\n",
    "                     Default=1000.\n",
    "         alpha:      The Dirichlet prior on the per-document topic distributions.\n",
    "                     Default: 50 / n_topics, based on Poldrack et al. (2012).\n",
    "         beta:       The Dirichlet prior on the per-topic word distribution.\n",
    "                     Default: 0.001, based on Poldrack et al. (2012).\n",
    "     \n",
    "     Returns:\n",
    "         weights_df: A pandas DataFrame derived from the MALLET\n",
    "                     output-doc-topics output file. Contains the weight assigned\n",
    "                     to each article for each topic, which can be used to select\n",
    "                     articles for topic-based meta-analyses (accepted threshold\n",
    "                     from Poldrack article is 0.001). [n_topics]+1 columns:\n",
    "                     'pmid' is the first column and the following columns are\n",
    "                     the topic names. The names of the topics match the names\n",
    "                     in df (e.g., topic_000).\n",
    "         keys_df:    A pandas DataFrame derived from the MALLET\n",
    "                     output-topic-keys output file. Contains the top [n_words]\n",
    "                     words for each topic, which can act as a summary of the\n",
    "                     topic's content. Two columns: 'topic' and 'terms'. The\n",
    "                     names of the topics match the names in weights (e.g.,\n",
    "                     topic_000).\n",
    "     \"\"\"\n",
    "     if abstracts.index.name != 'pmid':\n",
    "         abstracts.index = abstracts['pmid']\n",
    " \n",
    "     resdir = os.path.abspath(os.getcwd())\n",
    "     tempdir = os.path.join(resdir, 'topic_models')\n",
    "     absdir = os.path.join(tempdir, 'abstracts')\n",
    "     if not os.path.isdir(tempdir):\n",
    "         os.mkdir(tempdir)\n",
    "     \n",
    "     if alpha is None:\n",
    "         alpha = 50. / n_topics\n",
    "     \n",
    "     # Check for existence of MALLET and download if necessary\n",
    "     mallet_bin = os.path.join(resdir, 'mallet-2.0.7/bin/mallet')\n",
    "     if not os.path.isfile(mallet_bin):\n",
    "         print('MALLET toolbox not found. Downloading...')\n",
    "         cmd = ('curl -o {0}/mallet-2.0.7.tar.gz '\n",
    "                'http://mallet.cs.umass.edu/dist/mallet-2.0.7.tar.gz').format(resdir)\n",
    "         subprocess.call(cmd, shell=True)\n",
    "         cmd = 'cd {0}; tar zxf {0}/mallet-2.0.7.tar.gz'.format(resdir)\n",
    "         subprocess.call(cmd, shell=True)\n",
    "         os.remove(os.path.join(resdir, 'mallet-2.0.7.tar.gz'))\n",
    "     else:\n",
    "         print('MALLET toolbox found!')\n",
    "     \n",
    "     # Check for presence of abstract files and convert if necessary\n",
    "     if not os.path.isdir(absdir):\n",
    "         print('Abstracts folder not found. Creating abstract files...')\n",
    "         os.mkdir(absdir)\n",
    "         for pmid in abstracts.index.values:\n",
    "             abstract = abstracts.loc[pmid]['abstract']\n",
    "            # this was changed for py35 compatibility\n",
    "             with open(os.path.join(absdir, str(pmid)+'.txt'), 'w') as fo:\n",
    "                 fo.write(abstract)\n",
    "         \n",
    "     # Run MALLET topic modeling\n",
    "     print('Generating topics...')\n",
    "     import_str = ('{mallet} import-dir '\n",
    "                   '--input {absdir} '\n",
    "                   '--output {outdir}/topic-input.mallet '\n",
    "                   '--keep-sequence '\n",
    "                   '--remove-stopwords').format(mallet=mallet_bin,\n",
    "                                                absdir=absdir,\n",
    "                                                outdir=tempdir)\n",
    "     \n",
    "     train_str = ('{mallet} train-topics '\n",
    "                  '--input {out}/topic-input.mallet '\n",
    "                  '--num-topics {n_topics} '\n",
    "                  '--num-top-words {n_words} '\n",
    "                  '--output-topic-keys {out}/topic_keys.txt '\n",
    "                  '--output-doc-topics {out}/doc_topics.txt '\n",
    "                  '--num-iterations {n_iters} '\n",
    "                  '--output-model {out}/saved_model.mallet '\n",
    "                  '--random-seed 1 '\n",
    "                  '--alpha {alpha} '\n",
    "                  '--beta {beta}').format(mallet=mallet_bin, out=tempdir,\n",
    "                                          n_topics=n_topics, n_words=n_words,\n",
    "                                          n_iters=n_iters,\n",
    "                                          alpha=alpha, beta=beta)\n",
    " \n",
    "     subprocess.call(import_str, shell=True)\n",
    "     subprocess.call(train_str, shell=True)\n",
    "     \n",
    "     # Read in and convert doc_topics and topic_keys.\n",
    "     def clean_str(string):\n",
    "         return os.path.basename(os.path.splitext(string)[0])\n",
    "     \n",
    "     def get_sort(lst):\n",
    "         return [i[0] for i in sorted(enumerate(lst), key=lambda x:x[1])]\n",
    "     \n",
    "     topic_names = ['topic_{0:03d}'.format(i) for i in range(n_topics)]\n",
    "     \n",
    "     # doc_topics: Topic weights for each paper.\n",
    "     # The conversion here is pretty ugly at the moment.\n",
    "     # First row should be dropped. First column is row number and can be used\n",
    "     # as the index.\n",
    "     # Second column is 'file: /full/path/to/pmid.txt' <-- Parse to get pmid.\n",
    "     # After that, odd columns are topic numbers and even columns are the\n",
    "     # weights for the topics in the preceding column. These columns are sorted\n",
    "     # on an individual pmid basis by the weights.\n",
    "     n_cols = (2 * n_topics) + 1\n",
    "     dt_df = pd.read_csv(os.path.join(tempdir, 'doc_topics.txt'), \n",
    "                         delimiter='\\t', skiprows=1, header=None, index_col=0)\n",
    "     dt_df = dt_df[dt_df.columns[:n_cols]]\n",
    "     \n",
    "     # Get pmids from filenames\n",
    "     dt_df[1] = dt_df[1].apply(clean_str)\n",
    "     \n",
    "     # Put weights (even cols) and topics (odd cols) into separate dfs.\n",
    "     weights_df = dt_df[dt_df.columns[2::2]]\n",
    "     weights_df.index = dt_df[1]\n",
    "     weights_df.columns = range(n_topics)\n",
    "     \n",
    "     topics_df = dt_df[dt_df.columns[1::2]]\n",
    "     topics_df.index = dt_df[1]\n",
    "     topics_df.columns = range(n_topics)\n",
    "     \n",
    "     # Sort columns in weights_df separately for each row using topics_df.\n",
    "     sorters_df = topics_df.apply(get_sort, axis=1)\n",
    "     weights = weights_df.as_matrix()\n",
    "     sorters = sorters_df.as_matrix()\n",
    "     for i in range(sorters.shape[0]):  # there has to be a better way to do this.\n",
    "         weights[i, :] = weights[i, sorters[i, :]]\n",
    "     \n",
    "     # Define topic names (e.g., topic_000)\n",
    "     index = dt_df[1]\n",
    "     weights_df = pd.DataFrame(columns=topic_names, data=weights, index=index)\n",
    "     weights_df.index.name = 'pmid'\n",
    "     \n",
    "     # topic_keys: Top [n_words] words for each topic.\n",
    "     keys_df = pd.read_csv(os.path.join(tempdir, 'topic_keys.txt'),\n",
    "                           delimiter='\\t', header=None, index_col=0)\n",
    "     \n",
    "     # Second column is a list of the terms.\n",
    "     keys_df = keys_df[[2]]\n",
    "     keys_df.rename(columns={2: 'terms'}, inplace=True)\n",
    "     keys_df.index = topic_names\n",
    "     keys_df.index.name = 'topic'\n",
    "         \n",
    "     # Remove all temporary files (abstract files, model, and outputs).\n",
    "     # shutil.rmtree(tempdir)\n",
    "     \n",
    "     # Return article topic weights and topic keys.\n",
    "     return weights_df, keys_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_df, keys_df = topic_models(df)\n",
    "keys_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Looks great, Taylor! Now to test in a Docker container."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
